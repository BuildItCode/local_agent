#!/usr/bin/env node

const {exec, spawn} = require('child_process');
const fs = require('fs').promises;
const path = require('path');
const readline = require('readline');
const os = require('os');

// Handle fetch for older Node.js versions
let fetch;
if (typeof globalThis.fetch === 'undefined') {
    try {
        fetch = require('node-fetch');
    } catch (error) {
        console.error('âŒ node-fetch is required for Node.js < 18. Install with: npm install node-fetch');
        process.exit(1);
    }
} else {
    fetch = globalThis.fetch;
}

const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
});

function askQuestion(question) {
    return new Promise((resolve) => {
        rl.question(question, resolve);
    });
}

async function checkOllama() {
    return new Promise((resolve) => {
        exec('ollama --version', (error, stdout) => {
            if (error) {
                resolve(false);
            } else {
                console.log('âœ… Ollama is installed:', stdout.trim());
                resolve(true);
            }
        });
    });
}

async function installOllama() {
    const platform = os.platform();
    console.log(`\nğŸ”§ Installing Ollama for ${platform}...`);

    return new Promise((resolve, reject) => {
        let installCommand;

        switch (platform) {
            case 'darwin':
            case 'linux':
                installCommand = 'curl -fsSL https://ollama.ai/install.sh | sh';
                break;
            case 'win32':
                console.log('ğŸ“‹ For Windows, please download and install from: https://ollama.ai/download');
                console.log('After installation, restart your terminal and run this setup again.');
                resolve(false);
                return;
            default:
                console.log('âŒ Unsupported platform. Please visit https://ollama.ai for installation instructions.');
                resolve(false);
                return;
        }

        console.log(`Running: ${installCommand}`);
        console.log('This may take a few minutes...\n');

        const child = exec(installCommand, (error, stdout, stderr) => {
            if (error) {
                console.error('âŒ Installation failed:', error.message);
                reject(error);
            } else {
                console.log('\nâœ… Ollama installed successfully!');
                console.log('ğŸ“‹ You may need to restart your terminal or source your shell profile.');
                resolve(true);
            }
        });

        child.stdout.on('data', (data) => {
            process.stdout.write(data);
        });

        child.stderr.on('data', (data) => {
            process.stdout.write(data);
        });
    });
}

async function startOllama() {
    console.log('\nğŸš€ Starting Ollama server...');

    return new Promise((resolve) => {
        const child = spawn('ollama', ['serve'], {
            detached: true,
            stdio: 'ignore'
        });

        child.unref();

        setTimeout(async () => {
            const isRunning = await checkOllamaRunning();
            if (isRunning) {
                console.log('âœ… Ollama server started successfully');
                resolve(true);
            } else {
                console.log('âš ï¸  Ollama server may not have started properly');
                console.log('   You can manually start it with: ollama serve');
                resolve(false);
            }
        }, 3000);
    });
}

async function checkOllamaRunning() {
    try {
        const response = await fetch('http://localhost:11434/api/tags');
        if (response.ok) {
            console.log('âœ… Ollama is running');
            return true;
        }
    } catch (error) {
        console.log('âŒ Ollama is not running');
        return false;
    }
    return false;
}

async function pullModel(modelName) {
    return new Promise((resolve, reject) => {
        console.log(`ğŸ“¥ Pulling model: ${modelName}`);
        console.log('This may take a while...');

        const child = exec(`ollama pull ${modelName}`, (error, stdout, stderr) => {
            if (error) {
                reject(error);
            } else {
                console.log(`\nâœ… Model ${modelName} pulled successfully`);
                resolve(true);
            }
        });

        child.stdout.on('data', (data) => {
            process.stdout.write('.');
        });

        child.stderr.on('data', (data) => {
            process.stdout.write('.');
        });
    });
}

async function listModels() {
    try {
        const response = await fetch('http://localhost:11434/api/tags');
        const data = await response.json();
        const models = data.models || [];

        if (models.length === 0) {
            return [];
        }

        return models.map(model => ({
            name: model.name,
            size: model.size,
            sizeGB: (model.size / 1024 / 1024 / 1024).toFixed(1),
            modified: model.modified_at
        }));
    } catch (error) {
        return [];
    }
}

async function displayAvailableModels() {
    console.log('\nğŸŒŸ Popular models you can install:');

    const recommendedModels = [
        {
            name: 'qwen3',
            description: 'Best for coding tasks and programming',
            size: '~5.2GB',
            command: 'ollama pull qwen3:8b',
            useCase: 'Perfect for: JavaScript, Python, React, debugging, code generation'
        },
        {
            name: 'llama3.2',
            description: 'General purpose, excellent for conversations',
            size: '~2.0GB',
            command: 'ollama pull llama3.3:latest',
            useCase: 'Perfect for: Writing, analysis, general questions, explanations'
        },
        {
            name: 'Gemma3n',
            description: 'General purpose, balanced',
            size: '~7.5GB',
            command: 'ollama pull gemma3n:latest',
            useCase: 'Gemini 1.5 pro like performance'
        }
    ];

    recommendedModels.forEach((model, index) => {
        console.log(`\n${index + 1}. ğŸ¤– ${model.name} (${model.size})`);
        console.log(`   ğŸ“ ${model.description}`);
        console.log(`   ğŸ¯ ${model.useCase}`);
        console.log(`   ğŸ“¦ Install: ${model.command}`);
    });

    console.log('\nğŸ’¡ Recommendations:');
    console.log('   ğŸ”§ For coding projects â†’ qwen3');
    console.log('   ğŸ’¬ For general use â†’ llama3.2');
}

async function selectModel(models) {
    if (models.length === 0) {
        return null;
    }

    console.log('\nğŸ“‹ Currently installed models:');
    models.forEach((model, index) => {
        let description = '';
        const modelName = model.name.toLowerCase();

        if (modelName.includes('codellama') || modelName.includes('qwen3')) {
            description = ' - ğŸ”§ Best for coding';
        } else if (modelName.includes('llama')) {
            description = ' - ğŸ’¬ Great for conversations';
        } else if (modelName.includes('mistral')) {
            description = ' - âš¡ Fast and efficient';
        } else if (modelName.includes('phi')) {
            description = ' - ğŸƒ Lightweight and quick';
        } else if (modelName.includes('deepseek')) {
            description = ' - ğŸ§  Advanced coding specialist';
        } else if (modelName.includes('gemma')) {
            description = ' - ğŸ¢ Google\'s model';
        }

        const modified = new Date(model.modified).toLocaleDateString();
        console.log(`   ${index + 1}. ${model.name} (${model.sizeGB}GB)${description}`);
        console.log(`      ğŸ“… Last used: ${modified} \n`);
    });

    console.log(`\n   ${models.length + 1}. ğŸ“¥ Install a new model`);
    console.log(`   ${models.length + 2}. â­ï¸  Skip model selection`);

    while (true) {
        const choice = await askQuestion(`\nSelect a model (1-${models.length + 2}): `);
        const choiceNum = parseInt(choice);

        if (choiceNum >= 1 && choiceNum <= models.length) {
            return models[choiceNum - 1].name;
        } else if (choiceNum === models.length + 1) {
            return 'install_new';
        } else if (choiceNum === models.length + 2) {
            return 'skip';
        } else {
            console.log('âŒ Invalid choice. Please try again.');
        }
    }
}

async function installNewModel() {
    await displayAvailableModels();

    const modelName = await askQuestion('\nğŸ“¥ Enter the model name to install (e.g., qwen3): ');

    if (!modelName.trim()) {
        console.log('âŒ No model name provided');
        return null;
    }

    console.log(`\nğŸ¤” About to install "${modelName}"`);
    console.log('âš ï¸  This may take several minutes and use multiple GB of storage.');
    console.log('ğŸ’¡ Make sure you have a stable internet connection.');

    const confirm = await askQuestion(`\nProceed with installation? (y/N): `);

    if (confirm.toLowerCase() !== 'y' && confirm.toLowerCase() !== 'yes') {
        console.log('âŒ Installation cancelled');
        return null;
    }

    try {
        await pullModel(modelName.trim());
        return modelName.trim();
    } catch (error) {
        console.log(`\nâŒ Failed to install ${modelName}:`);
        console.log(`   ${error.message}`);
        console.log('\nğŸ’¡ Common issues:');
        console.log('   - Check your internet connection');
        console.log('   - Verify the model name is correct');
        console.log('   - Try: ollama list (to see available models)');
        return null;
    }
}

async function renameAgentFile(modelName) {
    try {
        // Extract base model name (remove version tags like :7b, :latest)
        const baseModelName = modelName.split(':')[0].toLowerCase();
        const newFileName = `${baseModelName}-agent.js`;

        console.log(`\nğŸ”„ Customizing agent for ${modelName}...`);

        // Check if terminal-agent.js exists
        const fs = require('fs');
        if (!fs.existsSync('terminal-agent.js')) {
            console.log('âš ï¸  terminal-agent.js not found, skipping rename');
            console.log('ğŸ’¡ Make sure you run this setup in the correct directory');
            return false;
        }

        // Check if target file already exists
        if (fs.existsSync(newFileName)) {
            const overwrite = await askQuestion(`ğŸ¤” ${newFileName} already exists. Overwrite? (y/N): `);
            if (overwrite.toLowerCase() !== 'y' && overwrite.toLowerCase() !== 'yes') {
                console.log('âŒ Skipping agent file rename');
                return false;
            }
        }

        // Rename the file
        await require('fs').promises.rename('terminal-agent.js', newFileName);
        console.log(`âœ… Agent renamed to: ${newFileName}`);

        // Update package.json to reflect new filename
        await updatePackageJson(newFileName, baseModelName, modelName);

        console.log(`\nğŸ¯ Your agent is now customized for ${modelName}!`);
        console.log(`\nğŸ“‹ Available commands:`);
        console.log(`   ğŸš€ node ${newFileName}              # Direct execution`);
        console.log(`   ğŸ“¦ npm start                       # Default start script`);
        console.log(`   ğŸ¯ npm run start:${baseModelName}     # Model-specific script`);
        if (baseModelName !== 'terminal') {
            console.log(`   ğŸ”— ${baseModelName}-ai                    # Global command (if installed)`);
        }

        return true;
    } catch (error) {
        console.log(`âŒ Could not rename agent file: ${error.message}`);
        return false;
    }
}

async function updatePackageJson(newFileName, baseModelName, fullModelName) {
    try {
        const fs = require('fs');

        // Read current package.json
        let packageJson;
        try {
            packageJson = JSON.parse(fs.readFileSync('package.json', 'utf8'));
        } catch (error) {
            console.log('âš ï¸  Could not read package.json, creating basic structure');
            packageJson = {
                name: 'terminal-llm-agent',
                version: '1.0.0',
                scripts: {},
                dependencies: {}
            };
        }

        // Update main entry point
        packageJson.main = newFileName;

        // Update bin entries
        const binName = `${baseModelName}-ai`;
        packageJson.bin = packageJson.bin || {};
        packageJson.bin[binName] = `./${newFileName}`;
        packageJson.bin['terminal-ai'] = `./${newFileName}`; // Keep original for compatibility

        // Add/update scripts
        if (!packageJson.scripts) {
            packageJson.scripts = {};
        }
        packageJson.scripts[`start:${baseModelName}`] = `node ${newFileName}`;
        packageJson.scripts.start = `node ${newFileName}`; // Update default start
        packageJson.scripts.setup = packageJson.scripts.setup || 'node setup.js';
        packageJson.scripts.debug = packageJson.scripts.debug || 'node debug-models.js';

        // Update description to include model
        const modelDisplayName = baseModelName.charAt(0).toUpperCase() + baseModelName.slice(1);
        packageJson.description = `AI assistant powered by ${modelDisplayName} with terminal and file system access using Ollama`;

        // Add model metadata
        packageJson.llmModel = {
            name: baseModelName,
            fullName: fullModelName,
            filename: newFileName,
            setupDate: new Date().toISOString(),
            version: packageJson.version || '1.0.0'
        };

        // Ensure keywords include model-specific terms
        if (!packageJson.keywords) {
            packageJson.keywords = [];
        }
        const modelKeywords = [baseModelName, 'llm', 'ai', 'terminal', 'ollama', 'assistant'];
        modelKeywords.forEach(keyword => {
            if (!packageJson.keywords.includes(keyword)) {
                packageJson.keywords.push(keyword);
            }
        });

        // Update author if not set
        if (!packageJson.author) {
            packageJson.author = `${modelDisplayName} Agent User`;
        }

        // Set license if not set
        if (!packageJson.license) {
            packageJson.license = 'MIT';
        }

        // Write updated package.json with proper formatting
        await require('fs').promises.writeFile('package.json', JSON.stringify(packageJson, null, 2));
        console.log(`âœ… Updated package.json for ${modelDisplayName}`);

        return true;
    } catch (error) {
        console.log(`âš ï¸  Could not update package.json: ${error.message}`);
        return false;
    }
}

async function setup() {
    console.log('ğŸš€ Terminal LLM Agent Setup Wizard');
    console.log('==================================');
    console.log('ğŸ¯ This wizard will help you set up and customize your AI agent\n');

    // Step 1: Check Ollama installation
    console.log('ğŸ“¦ Step 1: Checking Ollama installation...');
    const ollamaInstalled = await checkOllama();

    if (!ollamaInstalled) {
        console.log('\nâŒ Ollama is not installed.');
        console.log('ğŸ’¡ Ollama is required to run local AI models.');

        const shouldInstall = await askQuestion('ğŸ¤” Would you like to install Ollama now? (Y/n): ');

        if (shouldInstall.toLowerCase() === 'n' || shouldInstall.toLowerCase() === 'no') {
            console.log('\nğŸ“‹ To install Ollama manually:');
            console.log('   ğŸ macOS/Linux: curl -fsSL https://ollama.ai/install.sh | sh');
            console.log('   ğŸªŸ Windows: https://ollama.ai/download');
            console.log('\nğŸ”„ After installation, run this setup again: npm run setup');
            rl.close();
            return;
        }

        try {
            const installed = await installOllama();
            if (!installed) {
                console.log('\nğŸ’¡ Please install Ollama manually and run setup again.');
                rl.close();
                return;
            }
        } catch (error) {
            console.log('âŒ Installation failed. Please install manually from https://ollama.ai');
            rl.close();
            return;
        }
    }

    // Step 2: Check if Ollama is running
    console.log('\nğŸ”„ Step 2: Checking Ollama server status...');
    let ollamaRunning = await checkOllamaRunning();

    if (!ollamaRunning) {
        console.log('âš ï¸  Ollama server is not running.');
        const shouldStart = await askQuestion('ğŸ¤” Try to start Ollama server automatically? (Y/n): ');

        if (shouldStart.toLowerCase() !== 'n' && shouldStart.toLowerCase() !== 'no') {
            const started = await startOllama();
            if (!started) {
                console.log('\nğŸ“‹ Please start Ollama manually in another terminal:');
                console.log('   ollama serve');
                console.log('\nğŸ”„ Then run this setup again: npm run setup');
                rl.close();
                return;
            }
            ollamaRunning = true;
        } else {
            console.log('\nğŸ“‹ Please start Ollama manually:');
            console.log('   ollama serve');
            console.log('\nğŸ”„ Then run this setup again: npm run setup');
            rl.close();
            return;
        }
    }

    // Step 3: Model selection and installation
    console.log('\nğŸ¤– Step 3: Setting up your AI model...');
    const models = await listModels();

    let selectedModel = null;

    if (models.length === 0) {
        console.log('âŒ No AI models are currently installed.');
        console.log('ğŸ’¡ You need at least one model to use the agent.');

        await displayAvailableModels();

        const shouldInstall = await askQuestion('\nğŸ¤” Would you like to install a model now? (Y/n): ');

        if (shouldInstall.toLowerCase() !== 'n' && shouldInstall.toLowerCase() !== 'no') {
            selectedModel = await installNewModel();
            if (!selectedModel) {
                console.log('\nâš ï¸  No model was installed. You can install one later with:');
                console.log('   ollama pull codellama');
                console.log('   ollama pull llama2');
            }
        } else {
            console.log('\nğŸ’¡ You can install models later with: ollama pull <model-name>');
        }
    } else {
        console.log(`âœ… Found ${models.length} installed model(s)`);
        const choice = await selectModel(models);

        if (choice === 'install_new') {
            selectedModel = await installNewModel();
            if (!selectedModel && models.length > 0) {
                selectedModel = models[0].name; // Fallback to first available
            }
        } else if (choice === 'skip') {
            selectedModel = models[0].name; // Use first available
        } else {
            selectedModel = choice;
        }
    }

    // Step 4: Save configuration and customize agent
    if (selectedModel) {
        console.log('\nğŸ’¾ Step 4: Saving configuration...');

        const config = {
            model: selectedModel,
            ollamaUrl: 'http://localhost:11434',
            workingDirectory: process.cwd(),
            setupDate: new Date().toISOString(),
            version: '1.0.0'
        };

        try {
            await fs.writeFile('agent-config.json', JSON.stringify(config, null, 2));
            console.log(`âœ… Configuration saved with model: ${selectedModel}`);

            // Step 5: Rename and customize agent file
            console.log('\nğŸ¨ Step 5: Customizing agent for your model...');
            const renamed = await renameAgentFile(selectedModel);

            if (renamed) {
                console.log('âœ… Agent customization completed successfully!');
            } else {
                console.log('âš ï¸  Agent customization had issues, but you can still use the generic agent');
            }

        } catch (error) {
            console.log('âš ï¸  Could not save configuration file:', error.message);
        }
    } else {
        console.log('\nâš ï¸  No model selected. You can configure one later by running setup again.');
    }

    // Step 6: Final summary and next steps
    console.log('\nğŸ‰ Setup Complete!');
    console.log('==================');

    if (selectedModel) {
        const baseModelName = selectedModel.split(':')[0].toLowerCase();
        const agentFileName = `${baseModelName}-agent.js`;
        const modelDisplayName = baseModelName.charAt(0).toUpperCase() + baseModelName.slice(1);

        console.log(`\nâœ¨ Your ${modelDisplayName} agent is ready to use!`);
        console.log('\nğŸ“‹ Quick Start Commands:');
        console.log(`   ğŸ¯ node ${agentFileName}         # Direct execution`);
        console.log(`   ğŸ“¦ npm run start:${baseModelName}   # Model-specific script`);

        function getModelOptimization(modelName) {
            const optimizations = {
                codellama: 'Coding tasks, debugging, code generation, technical documentation',
                llama2: 'General conversation, writing, analysis, explanations',
                mistral: 'Balanced performance, quick responses, general tasks',
                phi: 'Fast responses, lightweight operations, resource efficiency',
                'deepseek-coder': 'Advanced coding, algorithms, code optimization',
                gemma: 'Google-optimized tasks, research, analysis'
            };

            return optimizations[modelName] || 'General purpose tasks';
        }

        console.log(`\nğŸ”§ Your agent specializes in: ${getModelOptimization(baseModelName)}`);

    } else {
        console.log('\nğŸ“‹ Next Steps:');
        console.log('   1. Install a model: ollama pull codellama');
        console.log('   2. Run setup again: npm run setup');
        console.log('   3. Start using: npm start');
    }

    console.log('\nğŸ’¡ Pro Tips:');
    console.log('   â€¢ Be specific in your requests for better results');
    console.log('   â€¢ The agent remembers context within each session');
    console.log('   â€¢ Use "help" in interactive mode for available commands');
    console.log('   â€¢ Check README.md for comprehensive documentation');

    console.log('\nğŸ†˜ Need Help?');
    console.log('   â€¢ Run: npm run debug (for diagnostics)');
    console.log('   â€¢ Check: README.md (for full documentation)');
    console.log('   â€¢ Try: examples/ (for usage patterns)');

    console.log(`\nğŸ¯ Ready to start? Run: ${selectedModel ? 'npm start' : 'npm run setup'}`);

    rl.close();
}

// Handle process interruption gracefully
process.on('SIGINT', () => {
    console.log('\n\nğŸ‘‹ Setup interrupted by user');
    console.log('ğŸ’¡ You can resume setup anytime by running: npm run setup');
    rl.close();
    process.exit(0);
});

process.on('SIGTERM', () => {
    console.log('\n\nğŸ‘‹ Setup terminated');
    rl.close();
    process.exit(0);
});

// Handle unexpected errors
process.on('uncaughtException', (error) => {
    console.error('\nğŸ’¥ Unexpected error during setup:', error.message);
    console.log('ğŸ”§ Please try running setup again: npm run setup');
    console.log('ğŸ†˜ If the problem persists, check:');
    console.log('   â€¢ Internet connection');
    console.log('   â€¢ Ollama installation');
    console.log('   â€¢ File permissions');
    rl.close();
    process.exit(1);
});

// Start the setup wizard
setup().catch((error) => {
    console.error('\nâŒ Setup failed:', error.message);
    console.log('\nğŸ”§ Troubleshooting tips:');
    console.log('   â€¢ Make sure you have internet connection');
    console.log('   â€¢ Check if Ollama is properly installed');
    console.log('   â€¢ Verify you have write permissions in this directory');
    console.log('   â€¢ Try running: npm run debug');
    rl.close();
    process.exit(1);
});

// Export functions for potential reuse
module.exports = {
    checkOllama,
    checkOllamaRunning,
    listModels,
    installOllama,
    startOllama,
    pullModel,
    renameAgentFile,
    updatePackageJson,
    setup
};